{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Reservoir Computing (QRC) for Swaption Pricing\n",
    "\n",
    "This notebook implements a Hybrid Quantum-Classical machine learning model to predict Swaption prices. It utilizes a Quantum Reservoir Computing (QRC) approach, where a fixed quantum circuit (the reservoir) maps input features into a high-dimensional Hilbert space, followed by a trainable classical readout layer.\n",
    "\n",
    "**Workflow:**\n",
    "1. Data Loading & Preprocessing\n",
    "2. Dimensionality Reduction (PCA)\n",
    "3. Quantum Reservoir State Generation\n",
    "4. Hybrid Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies\n",
    "Installing necessary packages for quantum simulation (`merlinquantum`, `qiskit`, `pennylane`) and data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install merlinquantum torch scikit-learn pandas numpy seaborn quantumreservoirpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Configuration\n",
    "Initializing global libraries and setting hyperparameters for the model (Modes, Photons, Epochs) and reproducible seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pennylane as qml\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "\n",
    "from quantumreservoirpy.reservoirs import Static, Incremental\n",
    "from qiskit_aer import AerSimulator\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from merlin.core import PhotonicBackend as Experiment\n",
    "\n",
    "from merlin import CircuitType\n",
    "from merlin import StatePattern\n",
    "# from merlin.core import AnsatzFactory\n",
    "\n",
    "import perceval as pcvl\n",
    "from merlin.algorithms import FeedForwardBlock\n",
    "from merlin.measurement.strategies import MeasurementStrategy\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import perceval as pcvl\n",
    "import merlin\n",
    "from merlin import QuantumLayer, ComputationSpace, LexGrouping, MeasurementStrategy\n",
    "from merlin.builder import CircuitBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Data Loading & Preprocessing\n",
    "# ==========================================\n",
    "torch.manual_seed(42)\n",
    "\n",
    "TRAIN_FILE = \"train.xlsx\" # For future use\n",
    "\n",
    "INCOMPLETE_PATH = 'price.xlsx'\n",
    "# Model Hyperparameters\n",
    "N_MODES =  8    # Number of optical modes (qubits equivalent)\n",
    "N_PHOTONS = 4       # Number of photons\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50         # Adjust based on how fast your computer is\n",
    "LEARNING_RATE = 0.005\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Initial Inspection\n",
    "Loading the raw training and pricing datasets to identify swaption tenors and maturities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA LOADING and indentification\n",
    "df_train = pd.read_excel(TRAIN_FILE)\n",
    "df_train[\"Date\"] = pd.to_datetime(df_train[\"Date\"], format=\"%d/%m/%Y\")\n",
    "\n",
    "# Identify swaption columns\n",
    "swaption_cols = [c for c in df_train.columns if c.startswith(\"Tenor\")]\n",
    "\n",
    "print(\"Training dataset loaded.\")\n",
    "print(\"Columns:\", len(swaption_cols), \"swaptions found.\")\n",
    "\n",
    "\n",
    "#Sample data\n",
    "df_sample = pd.read_excel(INCOMPLETE_PATH)\n",
    "df_sample[\"Date\"] = pd.to_datetime(df_sample[\"Date\"], format=\"%d/%m/%Y\")\n",
    "\n",
    "\n",
    "def parse_tenor_maturity(col):\n",
    "    m = re.search(r\"Tenor\\s*:\\s*([0-9.]+);\\s*Maturity\\s*:\\s*([0-9.]+)\", col)\n",
    "    return float(m.group(1)), float(m.group(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classical Baseline Models\n",
    "Defining classical architectures to serve as a performance benchmark against the Quantum model.\n",
    "1. **Linear Baseline:** A simple linear regression on raw features.\n",
    "2. **PCA Baseline:** A linear regression using PCA-reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Baseline Linear Model\n",
    "class LinearModelBaseline(nn.Module):\n",
    "    def __init__(self, image_size, num_classes=10):\n",
    "        super(LinearModelBaseline, self).__init__()\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # Classical part\n",
    "        self.classifier = nn.Linear(image_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Data is already flattened\n",
    "        output = self.classifier(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearModel with PCA\n",
    "class LinearModelPCA(nn.Module):\n",
    "    def __init__(self, n_swaps, pca_components):\n",
    "        super(LinearModelPCA, self).__init__()\n",
    "        self.n_swaps = n_swaps\n",
    "        self.pca_components = pca_components\n",
    "\n",
    "        # Classical part\n",
    "        self.classifier = nn.Linear(\n",
    "            n_swaps+pca_components, n_swaps\n",
    "        )\n",
    "    def forward(self, x, x_pca):\n",
    "        # Data is already flattened, just concatenate\n",
    "        combined_features = torch.cat((x, x_pca), dim=1)\n",
    "        output = self.classifier(combined_features)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processing Pipeline\n",
    "Here we define the `SwaptionDataset` class and the `load_and_process_data` function.\n",
    "\n",
    "**Steps:**\n",
    "1. **Target Engineering:** We predict the *difference* in price ($y_{t+1} - y_t$).\n",
    "2. **Scaling:** Inputs are scaled to $[0, \\pi]$ (for quantum angle encoding) and Targets are standardized.\n",
    "3. **Splitting:** Data is split into training and testing sets based on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "TRAIN_FILE = \"train.xlsx\"\n",
    "SAMPLE_FILE = \"sample_Simulated_Swaption_Price.xlsx\"\n",
    "\n",
    "#DATASET\n",
    "class SwaptionDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def load_and_process_data(filepath):\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    df = pd.read_excel(filepath)\n",
    "\n",
    "    # Parse dates\n",
    "    df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "    df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "    # Extract features (all columns except Date)\n",
    "    feature_cols = [c for c in df.columns if c != 'Date']\n",
    "    data_values = df[feature_cols].values\n",
    "\n",
    "    print(f\"Data shape: {data_values.shape} (Days, Surface Points)\")\n",
    "\n",
    "# Create Input (X) and Target (y)\n",
    "\n",
    "    # --- SPLIT FIRST, THEN SCALE ---\n",
    "    # We define the split index manually or using sklearn first\n",
    "\n",
    "    X_raw = data_values[:-1]\n",
    "    y_raw = data_values[1:]\n",
    "    split_idx = int(len(X_raw) * 0.75)\n",
    "\n",
    "    # y_target is now the DIFFERENCE (Change in price)\n",
    "    y_diff = y_raw - X_raw \n",
    "\n",
    "    # Scale X (Input) normally\n",
    "    scaler_x = MinMaxScaler(feature_range=(0, 1)) # Use 0,1 for stability\n",
    "    X_scaled = scaler_x.fit_transform(X_raw)\n",
    "\n",
    "    # Scale y_diff (Target) - Use StandardScaler for differences as they are centered around 0\n",
    "    scaler_y = StandardScaler()\n",
    "    y_diff_scaled = scaler_y.fit_transform(y_diff)\n",
    "    \n",
    "    X_train_raw, X_test_raw = X_raw[:split_idx], X_raw[split_idx:]\n",
    "    y_train_raw, y_test_raw = y_raw[:split_idx], y_raw[split_idx:]\n",
    "\n",
    "    # Define Scaler for the raw input/output (The curve data)\n",
    "    # We fit ONLY on training data\n",
    "    scaler = MinMaxScaler(feature_range=(0, np.pi))\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "    # Transform test using train statistics\n",
    "    X_test_scaled = scaler.transform(X_test_raw)\n",
    "    \n",
    "    y_train_scaled = scaler.transform(y_train_raw)\n",
    "    y_test_scaled = scaler.transform(y_test_raw)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, scaler\n",
    "\n",
    "# Load data\n",
    "if not os.path.exists(TRAIN_FILE):\n",
    "    raise FileNotFoundError(f\"Could not find {TRAIN_FILE}. Please ensure it is in the folder.\")\n",
    "\n",
    "X_train, X_test, y_train, y_test, scaler = load_and_process_data(TRAIN_FILE)\n",
    "\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(SwaptionDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(SwaptionDataset(X_test, y_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Testing samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quantum Reservoir Definitions\n",
    "\n",
    "### Option A: Photonic Reservoir (Merlin)\n",
    "Definition of a photonic quantum layer using the `merlinquantum` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of QuantumReservoir class - quantum layer applying on pca, and linear classifier on input and pca\n",
    "class QuantumReservoir(nn.Module):\n",
    "    def __init__(self, n_swaps, pca_components, n_modes, n_photons):\n",
    "        #input_dim?\n",
    "        super(QuantumReservoir, self).__init__()\n",
    "        self.n_swaps = n_swaps\n",
    "        self.pca_components = pca_components\n",
    "        self.n_modes = n_modes\n",
    "        self.n_photons = n_photons\n",
    "\n",
    "        # Quantum part (non-trainable reservoir)\n",
    "        self.quantum_layer = self._create_quantum_reservoir(\n",
    "            pca_components, n_modes, n_photons\n",
    "        )\n",
    "\n",
    "        q_dim = self.quantum_layer.output_size\n",
    "\n",
    "        # Classical part\n",
    "        self.classifier = nn.Linear(\n",
    "            q_dim+n_swaps, n_swaps)\n",
    "\n",
    "        print(f\"\\nQuantum Reservoir Created:\")\n",
    "        print(f\"  Input size (PCA components): {pca_components}\")\n",
    "        print(f\"  Quantum output size: {self.quantum_layer.output_size}\")\n",
    "        print(f\"  Total features to classifier: {n_swaps + self.quantum_layer.output_size}\")\n",
    "\n",
    "    def _create_quantum_reservoir(self, input_size, n_modes, n_photons):\n",
    "        \"\"\"Create quantum layer with Series circuit in reservoir mode.\"\"\"\n",
    "        builder = CircuitBuilder(n_modes=n_modes)\n",
    "\n",
    "        builder.add_superpositions(depth=3)  # Deeper\n",
    "        builder.add_angle_encoding()\n",
    "        builder.add_rotations(trainable=False)  # Random fixed phases\n",
    "        builder.add_superpositions(depth=3)\n",
    "        builder.add_angle_encoding()\n",
    "        quantum_layer = QuantumLayer(input_size=input_size, n_photons=n_photons, builder=builder,\n",
    "                                     measurement_strategy=MeasurementStrategy.PROBABILITIES)        \n",
    "        # builder.add_superpositions(depth=2)\n",
    "        # builder.add_angle_encoding()\n",
    "        # for _ in range(3):  # 3x deeper adaptive layers\n",
    "        #     builder.add_adaptive_layers(num_layers=2, trainable=False)  # Random fixed params\n",
    "        # builder.add_interferometers(depth=3)  # Multi-layer interferometry\n",
    "        quantum_layer = QuantumLayer(input_size=input_size, n_photons=n_photons, builder=builder,\n",
    "                                     measurement_strategy=merlin.MeasurementStrategy.PROBABILITIES,\n",
    "                                     computation_space=ComputationSpace.FOCK)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        return quantum_layer\n",
    "\n",
    "    def forward(self, x, x_pca):\n",
    "        # Process the PCA-reduced input through quantum layer\n",
    "        quantum_output = self.quantum_layer(x_pca)\n",
    "\n",
    "        # Concatenate original image features with quantum output\n",
    "        combined_features = torch.cat((x, quantum_output), dim=1)\n",
    "\n",
    "        # Final classification\n",
    "        output = self.classifier(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Gate-Based Reservoir (Qiskit/QRPy)\n",
    "Definition of a gate-based superconducting circuit reservoir using `qiskit`. This class implements:\n",
    "* **Data Re-uploading:** Encoding features into rotation gates ($R_y$) at multiple layers.\n",
    "* **Entanglement:** CNOT gates to spread information across qubits.\n",
    "* **Echo Dynamics:** Fixed random unitary evolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Reservoir Class Structure\n",
    "class DeepStaticReservoir(Static):\n",
    "    def __init__(self, n_qubits, n_layers, ry_angs, backend):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        self.ry_angs = ry_angs\n",
    "        super().__init__(n_qubits=n_qubits, backend=backend)\n",
    "\n",
    "    def before(self, circuit):\n",
    "        circuit.h(range(self.n_qubits))\n",
    "\n",
    "    def during(self, circuit, timestep, reservoir_number):\n",
    "        if np.isscalar(timestep):\n",
    "            timestep = [timestep]\n",
    "\n",
    "        # RE-UPLOADING ARCHITECTURE\n",
    "        # Instead of encoding once at the start, we interleave encoding and processing\n",
    "        \n",
    "        for l in range(self.n_layers):\n",
    "            # 1. ENCODING STEP (Data Re-uploading)\n",
    "            # We map the features to the qubits again in every layer\n",
    "            for i, feature_val in enumerate(timestep):\n",
    "                qubit_idx = i % self.n_qubits\n",
    "                # Use a specific scaling factor (e.g., 2.0 to cover full Bloch sphere)\n",
    "                # Input assumed to be [-1, 1] or [0, 1] from scaler\n",
    "                # Remove the generic * np.pi if your input is already [0, pi]\n",
    "                # If input is [0, 1], use feature_val * 2 * np.pi\n",
    "                circuit.ry(feature_val, qubit_idx) \n",
    "\n",
    "            # 2. PROCESSING STEP (Weights + Entanglement)\n",
    "            for q in range(self.n_qubits):\n",
    "                circuit.rx(self.ry_angs[l, q], q) # RX gives orthogonal rotation to RY\n",
    "            \n",
    "            # Entanglement (CNOTs)\n",
    "            for q in range(self.n_qubits - 1):\n",
    "                circuit.cx(q, q + 1)\n",
    "            if self.n_qubits > 1:\n",
    "                circuit.cx(self.n_qubits - 1, 0)\n",
    "\n",
    "        # --- Reservoir Dynamics (Your existing \"Echo\" layers) ---\n",
    "        for l in range(self.n_layers):\n",
    "            for q in range(self.n_qubits):\n",
    "                circuit.ry(self.ry_angs[l, q], q)\n",
    "            # Full linear entanglement\n",
    "            for q in range(self.n_qubits - 1):\n",
    "                circuit.cx(q, q + 1)\n",
    "            # Circular entanglement\n",
    "            if self.n_qubits > 1:\n",
    "                circuit.cx(self.n_qubits - 1, 0)\n",
    "\n",
    "    def after(self, circuit):\n",
    "        # circuit.measure_all()\n",
    "        circuit.save_statevector()\n",
    "\n",
    "class QRPyWrapper:\n",
    "    def __init__(self, n_qubits, n_layers, backend):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.backend = backend\n",
    "        np.random.seed(42)\n",
    "        self.ry_angs = np.pi * np.random.rand(n_layers, n_qubits)\n",
    "        \n",
    "        self.reservoir = DeepStaticReservoir(\n",
    "            n_qubits=n_qubits, n_layers=n_layers, \n",
    "            ry_angs=self.ry_angs, backend=backend\n",
    "        )\n",
    "\n",
    "    def get_states(self, X_pca_data):\n",
    "        input_timeseries = [row for row in X_pca_data]\n",
    "        print(\"Running Quantum Simulation (Exact Statevector)...\")\n",
    "        # Run without shots (or shots=1 for statevector backend, but QRPy might handle it)\n",
    "        # Note: We rely on the backend being configured for statevectors\n",
    "        raw_states = self.reservoir.run(timeseries=input_timeseries) \n",
    "        return self._process_states(raw_states)\n",
    "\n",
    "    def _process_states(self, res_states):\n",
    "        # res_states will now contain Statevector objects if using save_statevector\n",
    "        dim = 2**self.n_qubits\n",
    "        feats = []\n",
    "        \n",
    "        for state in res_states:\n",
    "            # Check if it is a Qiskit Statevector object\n",
    "            if hasattr(state, 'data'): \n",
    "                # Get probabilities: |amplitude|^2\n",
    "                probs = np.abs(state.data)**2\n",
    "            elif isinstance(state, np.ndarray):\n",
    "                probs = np.abs(state)**2\n",
    "            else:\n",
    "                # Fallback for dictionary counts (legacy)\n",
    "                continue \n",
    "                \n",
    "            # Truncate or Pad to ensure size consistency\n",
    "            if len(probs) > dim:\n",
    "                probs = probs[:dim]\n",
    "            elif len(probs) < dim:\n",
    "                probs = np.pad(probs, (0, dim - len(probs)))\n",
    "                \n",
    "            feats.append(probs)\n",
    "            \n",
    "        return np.array(feats, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. State Processing Utilities\n",
    "Helper functions to extract probability distributions (statevectors) from the quantum backend and normalize them for the classical readout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_states(res_states, shots=1024, max_dim=256):\n",
    "    \"\"\"Safe extract: handles Counter or ndarray; normalize probs\"\"\"\n",
    "    feats = []\n",
    "    for state in res_states:\n",
    "        if isinstance(state, dict) or hasattr(state, 'values'):  # Counter/dict\n",
    "            counts = np.array(list(state.values()))\n",
    "        elif isinstance(state, np.ndarray):\n",
    "            counts = state.astype(int)\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected state type: {type(state)}\")\n",
    "        \n",
    "        probs = counts / np.sum(counts) if np.sum(counts) > 0 else np.zeros_like(counts)\n",
    "        feats.append(probs[:max_dim])\n",
    "    return np.array(feats, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Extraction (PCA)\n",
    "To fit high-dimensional market data into limited quantum modes/qubits, we apply Principal Component Analysis (PCA). The features are then rescaled to $[0, 2\\pi]$ for angle encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=N_MODES*4\n",
    "M=N_MODES\n",
    "N=N_PHOTONS\n",
    "\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "\n",
    "# Note: Data is already flattened\n",
    "X_train_flat = X_train\n",
    "X_test_flat = X_test\n",
    "\n",
    "X_train_pca_raw = pca.fit_transform(X_train_flat) # Assuming X_train_flat from Fix A\n",
    "X_test_pca_raw = pca.transform(X_test_flat)\n",
    "\n",
    "scaler_pca = MinMaxScaler(feature_range=(0, 2*np.pi))\n",
    "X_train_pca = torch.FloatTensor(scaler_pca.fit_transform(X_train_pca_raw))\n",
    "X_test_pca = torch.FloatTensor(scaler_pca.transform(X_test_pca_raw))\n",
    "\n",
    "print(f\"PCA Features Scaled Min: {X_train_pca.min()}, Max: {X_train_pca.max()}\")\n",
    "# Expected output: Min: 0.0, Max: 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Reservoir State Generation\n",
    "We pre-compute the quantum reservoir states. Since the reservoir weights are fixed (non-trainable), we can run the quantum simulation once for the entire dataset and cache the results to speed up training.\n",
    "\n",
    "* **Backend:** `AerSimulator` (Statevector method).\n",
    "* **Layers:** 10 layers of re-uploading and entanglement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Initialize Wrapper ---\n",
    "n_qubits = 8\n",
    "n_layers = 10 # Reduced from 20 for speed during debugging\n",
    "backend = AerSimulator(method='statevector')\n",
    "qr_wrapper = QRPyWrapper(n_qubits, n_layers, backend)\n",
    "\n",
    "\n",
    "# --- 2. Pre-compute Reservoir States ---\n",
    "# Ensure inputs are numpy\n",
    "X_train_np = X_train_pca.numpy()\n",
    "X_test_np = X_test_pca.numpy()\n",
    "\n",
    "print(\"Generating Training States...\")\n",
    "feats_train_np = qr_wrapper.get_states(X_train_np)\n",
    "\n",
    "print(\"Generating Test States...\")\n",
    "feats_test_np = qr_wrapper.get_states(X_test_np)\n",
    "\n",
    "# Convert to Tensor\n",
    "feats_train = torch.FloatTensor(feats_train_np)\n",
    "feats_test = torch.FloatTensor(feats_test_np)\n",
    "\n",
    "print(f\"Reservoir Output Shape: {feats_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hybrid Model Initialization\n",
    "We define the `HybridReadout` model, which concatenates the original classical features (or PCA features) with the high-dimensional quantum reservoir features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridReadout(nn.Module):\n",
    "    def __init__(self, input_dim, reservoir_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Input: Original Features (or PCA) + Quantum Features\n",
    "        self.layer = nn.Linear(input_dim + reservoir_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, q_feats):\n",
    "        # Concatenate Classical and Quantum features\n",
    "        combined = torch.cat((x, q_feats), dim=1)\n",
    "        return self.layer(combined)\n",
    "\n",
    "# Initialize Model\n",
    "# input_dim can be X_train_tensor.shape[1] (original) or X_train_pca.shape[1] (PCA)\n",
    "# Based on your previous code, you combined PCA + Quantum\n",
    "model_input_dim = X_train_pca.shape[1] \n",
    "reservoir_dim = feats_train.shape[1]\n",
    "output_dim = 224 \n",
    "\n",
    "# Correctly initialize Baseline models with 224 outputs\n",
    "linear_model = LinearModelBaseline(image_size=X_train_flat.shape[1], num_classes=output_dim)\n",
    "pca_model = LinearModelPCA(n_swaps=output_dim, pca_components=n_components)\n",
    "\n",
    "# Check dimensions\n",
    "print(f\"Linear Model Output Dim: {linear_model.classifier.out_features}\") \n",
    "qrc_model = HybridReadout(model_input_dim, reservoir_dim, output_dim)\n",
    "optimizer = torch.optim.Adam(qrc_model.parameters(), lr=0.0005)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Training & Evaluation\n",
    "The final training loop for the hybrid model.\n",
    "* **Optimizer:** Adam\n",
    "* **Loss Function:** MSE\n",
    "* **Metric:** $R^2$ Score on the testing set (inverse transformed to original price scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataLoaders that include the pre-computed reservoir features\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_pca, feats_train, torch.Tensor(y_train))\n",
    "# IMPORTANT: Shuffle MUST be True here for training, but we already aligned feats with X\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    qrc_model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_pca, batch_qfeats, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        preds = qrc_model(batch_pca, batch_qfeats)\n",
    "        loss = loss_fn(preds, batch_y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# --- Evaluation Function with Fix ---\n",
    "def evaluate_model(model, X_pca, Q_feats, y_true, scaler):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_scaled = model(X_pca, Q_feats).numpy()\n",
    "        y_true_scaled = y_true # Assuming y_true is passed as scaled tensor/numpy\n",
    "    \n",
    "    # 1. Inverse Transform Preds\n",
    "    # The scaler expects 224 features. \n",
    "    # If the model predicts 224, direct inverse.\n",
    "    if preds_scaled.shape[1] == scaler.n_features_in_:\n",
    "        preds_real = scaler.inverse_transform(preds_scaled)\n",
    "        # y_true might be tensor, convert to numpy\n",
    "        if isinstance(y_true_scaled, torch.Tensor):\n",
    "            y_true_scaled = y_true_scaled.numpy()\n",
    "        y_real = scaler.inverse_transform(y_true_scaled)\n",
    "    else:\n",
    "        # Fallback for dimension mismatch (padding)\n",
    "        # This shouldn't happen if output_dim=224, but good for safety\n",
    "        padded_preds = np.zeros((preds_scaled.shape[0], scaler.n_features_in_))\n",
    "        padded_preds[:, :preds_scaled.shape[1]] = preds_scaled\n",
    "        preds_real = scaler.inverse_transform(padded_preds)[:, :preds_scaled.shape[1]]\n",
    "        \n",
    "        padded_y = np.zeros((y_true_scaled.shape[0], scaler.n_features_in_))\n",
    "        padded_y[:, :y_true_scaled.shape[1]] = y_true_scaled\n",
    "        y_real = scaler.inverse_transform(padded_y)[:, :y_true_scaled.shape[1]]\n",
    "\n",
    "    # Metrics\n",
    "    mse = mean_squared_error(y_real, preds_real)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_real, preds_real)\n",
    "    \n",
    "    print(f\"Test MSE: {mse:.8f} | RMSE: {rmse:.4f} | R2: {r2:.4f}\")\n",
    "    return preds_real, y_real\n",
    "\n",
    "# Run Eval\n",
    "print(\"\\nEvaluating...\")\n",
    "preds_real, y_real = evaluate_model(qrc_model, X_test_pca, feats_test, y_test, scaler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}